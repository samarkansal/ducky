{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7a87fb7f6c8cf0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"banner-5-coding-with-ai.png\" width=\"100%\">\n",
    "<br>\n",
    "\n",
    "# **Understanding Embeddings in Large Language Models (LLM)**\n",
    "\n",
    "---\n",
    "\n",
    "Embeddings are a foundational concept in natural language processing and machine learning. In the context of a language model, they convert words, sentences, or entire documents into numerical vectors of fixed size. These vectors capture the semantic meaning of the input text. In this notebook, we'll dive deep into what embeddings mean for an LLM like GPT (Generative Pre-trained Transformer).\n",
    "\n",
    "---\n",
    "\n",
    "**1. Introduction to Embeddings**\n",
    "\n",
    "At its core, an embedding is a mapping from discrete objects (such as words) to vectors of real numbers.\n",
    "\n",
    "```python\n",
    "# Sample Word to Vector Representation (Hypothetical)\n",
    "word = \"computer\"\n",
    "vector_representation = [0.12, -0.58, 0.91, ...]  # a long list of numbers\n",
    "```\n",
    "\n",
    "This vector representation is useful because:\n",
    "- Vectors can be input into neural networks.\n",
    "- Semantically similar words will have similar vector representations.\n",
    "- They allow for efficient computations to measure similarity, perform arithmetic operations, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**2. How LLMs Use Embeddings**\n",
    "\n",
    "LLMs typically utilize embeddings in two main phases:\n",
    "\n",
    "1. **Embedding Layer (Input)**: Convert words/tokens into vectors.\n",
    "2. **Contextual Embeddings (Hidden States)**: Capture contextual information as the model processes sequences.\n",
    "\n",
    "The magic of LLMs like GPT is that they don't just use a static embedding for each word; the embedding changes based on the context!\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**3. Exploring Semantic Relationships**\n",
    "\n",
    "Embeddings can capture various semantic relationships. For example, the famous analogy \"man is to king as woman is to queen\" can be represented through vector arithmetic.\n",
    "\n",
    "```python\n",
    "# Hypothetical representation\n",
    "vector('king') - vector('man') + vector('woman') â‰ˆ vector('queen')\n",
    "```\n",
    "\n",
    "This showcases the depth and richness of information present in the embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "Embeddings are a powerful way to represent text numerically, capturing rich semantic meanings in compact vectors. Through LLMs, these embeddings aren't just static but evolve based on context, providing a deep understanding of language nuances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321812bda7f823",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using Embeddings to answer questions about a local document\n",
    "\n",
    "One of the ways we can use embeddings is to answer questions about a local document. For example, given a PDF file, we can extract the content, split it into smaller documents (e.g., pages), embed them, and then perform semantic search to answer questions about the document.\n",
    "\n",
    "To process a PDF and extract information for embedding, you would typically use the `PyPDF2` library. Then, you can use FAISS and OpenAI's embeddings (or embeddings from any other model) to do a semantic search. Below is a more detailed and realistic example of how to do this.  We will use Scikit-Learn library's nearest neighbor algorithm to perform semantic search on the embeddings we get from OpenAI's API.\n",
    "\n",
    "---\n",
    "\n",
    "**Embedding and Semantic Search on PDF Content using Scikit-Learn and OpenAI**\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1:** Import Necessary Libraries\n",
    "\n",
    "First, let's import the required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "190c9c7fddcb2f06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T23:46:22.188226Z",
     "start_time": "2023-09-04T23:46:22.131084Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "import PyPDF2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eea296306397c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Step 2:** Extract Content from PDF\n",
    "\n",
    "We'll use `PyPDF2` to extract content from the given PDF file, \"ThePragmaticProgrammer.pdf\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbd65cb8cfd47222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T23:46:22.948490Z",
     "start_time": "2023-09-04T23:46:22.134129Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \" \".join([reader.pages[i].extract_text() for i in range(len(reader.pages))])\n",
    "    return text\n",
    "\n",
    "pdf_content = extract_text_from_pdf(\"ThePragmaticProgrammer.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3703e74313e55",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3:** Split PDF Content into Documents\n",
    "\n",
    "For simplicity, we're blindly chunking the document \n",
    "into smaller documents (in this case, approximately 1500-token chunks with a 50 token overlap). \n",
    "In a real-world scenario, you would want to split the document into meaningful chunks (e.g., paragraphs, sections, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741db631c9332d6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import chunk_prompt\n",
    "\n",
    "documents = chunk_prompt(pdf_content, chunk_size=500, overlap=50)\n",
    "\n",
    "print(f\"Number of Split Documents: {len(documents)}\")\n",
    "print(f\"First Document length: {len(documents[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633178eaa20f5282",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "**Step 4:** Embed the Documents\n",
    "\n",
    "We'll obtain embeddings for each document. This can be done in batches of up to 2048 documents at a time. We'll use OpenAI's `text-embedding-ada-002` model to embed the documents. This model is trained on a large corpus of text and is able to capture rich semantic information.  We will do 20 documents at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aefa7cdcfb938c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# openai credentials\n",
    "openai.api_base = 'http://aitools.cs.vt.edu:7860/openai/v1'\n",
    "openai.api_key = 'aitools'\n",
    "\n",
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 20  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(documents), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = documents[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end - 1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": documents, \"embedding\": embeddings})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54fd7904bd8a919",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are using a data frame to visualize the data here.\n",
    "\n",
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0ad26efce207b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's use SciKit-Learn's Nearest Neighbors algorithm to perform semantic search on the embeddings. We'll use the `ball_tree` algorithm, which is a fast implementation of the k-nearest neighbors algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7af7624379220aea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-04T23:46:34.617421Z",
     "start_time": "2023-09-04T23:46:34.597887Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2e8b320738b8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's now try to ask some questions about the document and see if we can get the right answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec625b7d2bc3b5c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"When should I catch an Exception?\"\n",
    "# Example new embedding\n",
    "response = openai.Embedding.create(model=EMBEDDING_MODEL, input=query)\n",
    "query_embedding = np.array(response[\"data\"][0][\"embedding\"]).reshape(1, -1)\n",
    "print(query_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254db6822af18bad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(query_embedding)\n",
    "\n",
    "print(\"Nearest Neighbors Indices:\", indices)\n",
    "print(\"Distances:\", distances)\n",
    "\n",
    "count = 0\n",
    "for idx in indices[0]:\n",
    "    print(\"\"\"[{idx}]@{distance} {doc}\"\"\".format(idx=idx, distance=distances[0][count], doc=documents[idx].replace(\"\\n\", \" \")))\n",
    "    print(\"-\" * 100)\n",
    "    print(\"\\n\\n\")\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c9e7b81a309b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How do we know if the semantic search did a good job?\n",
    "We can use a context + question prompt to OpenAI to see if the LLM can explain the question using the context. If it can, then we know that the semantic search did a good job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5933a61bee8f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import converse\n",
    "prompt_template = \"\"\"\n",
    "Answer the following question using the context provided:\n",
    "%Question: \n",
    "```\n",
    "{question}\n",
    "``` \n",
    "%Context: \n",
    "```\n",
    "{context}\n",
    "```\n",
    "\"\"\"\n",
    "for idx in indices[0]:\n",
    "    messages = []\n",
    "    prompt = prompt_template.format(question=query, context=documents[idx])\n",
    "    messages, response = converse(messages, prompt)\n",
    "    print(f\"\"\" [{idx}]: Explanation: {response}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ea183611df91f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "\n",
    "We've demonstrated how to extract content from a PDF, split it into smaller documents, embed them, and perform semantic search using Scikit-Learn and OpenAI's embeddings. This can be a powerful way to search through large documents or even collections of documents.\n",
    " \n",
    "Tip: For practical deployment, always consider factors like the size of your dataset, the frequency of queries, and the desired \n",
    "latency. In many real-world scenarios, batching operations, caching frequent queries, or using more specialized search libraries can significantly enhance performance and user experience. Furthermore, periodically updating embeddings can ensure that your semantic search remains relevant as the underlying content or context evolves.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
